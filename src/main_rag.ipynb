{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372e426d",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93be22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4af845",
   "metadata": {},
   "source": [
    "# Get Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e2c761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Loading embedding model utility...\n",
      "INFO: Initializing HuggingFaceEmbeddings model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faiisu/Documents/Project/RAGdevelop/src/embedded_utils.py:11: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/Users/faiisu/Documents/Project/RAGdevelop/myenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from embedded_utils import get_embedding_model\n",
    "embedding_model = get_embedding_model()\n",
    "llm = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "688866ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_fin_dict = {\n",
    "    'Risk Management': 'q-fin.RM',\n",
    "    'Computational Finance': 'q-fin.CP',\n",
    "    'Statistical Finance': 'q-fin.ST',\n",
    "    'Trading and Market Microstructure': 'q-fin.TR',\n",
    "    'Economics': 'q-fin.EC',\n",
    "    'General Finance': 'q-fin.GN',\n",
    "    'Mathematical Finance': 'q-fin.MF',\n",
    "    'Portfolio Management': 'q-fin.PM',\n",
    "    'Pricing of Securities': 'q-fin.PR'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50105d85",
   "metadata": {},
   "source": [
    "# Get Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6340edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_db_path = \"../vectorDB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1bdcfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../vectorDB/q-fin.topic\n",
      "Loading vector store from: ../vectorDB/q-fin.topic\n",
      "Vector store loaded successfully.\n",
      "Number of documents in store: 9\n"
     ]
    }
   ],
   "source": [
    "persist_directory = 'q-fin.topic'\n",
    "persist_directory = root_db_path + \"/\" + persist_directory\n",
    "print(persist_directory)\n",
    "print(f\"Loading vector store from: {persist_directory}\")\n",
    "\n",
    "try:\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embedding_model\n",
    "    )\n",
    "    print(\"Vector store loaded successfully.\")\n",
    "    # Optional: Check the number of documents in the store\n",
    "    print(f\"Number of documents in store: {vectorstore._collection.count()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading vector store: {e}\")\n",
    "    # Handle error, maybe the directory doesn't exist or is corrupted\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75bdefb",
   "metadata": {},
   "source": [
    "# Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8a48432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever created successfully.\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 3}\n",
    ")\n",
    "\n",
    "print(\"Retriever created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88743d",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert assistant for answering questions.\n",
    "Use only the following context to answer the question at the end.\n",
    "If you don't know the answer from the context, just say that you don't know.\n",
    "Do not make up an answer.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387a37b",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# --- 5. Build the RAG Chain using LCEL ---\n",
    "\n",
    "# This function formats the retrieved documents into a single string\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# The core RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chain created successfully. Ready to answer questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794204dc",
   "metadata": {},
   "source": [
    "# Chain runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 6. Run the Chain ---\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        question = input(\"Ask a question (or type 'exit' to quit): \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        # Invoke the chain to get the answer\n",
    "        answer = rag_chain.invoke(question)\n",
    "        \n",
    "        print(\"\\n--- Answer ---\\n\")\n",
    "        print(answer)\n",
    "        print(\"\\n--------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
