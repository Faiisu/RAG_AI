{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372e426d",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93be22f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4af845",
   "metadata": {},
   "source": [
    "# Get Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dac1f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1e2c761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Initializing HuggingFaceEmbeddings model: all-MiniLM-L6-v2\n",
      "LLM configured to use OpenRouter with model: qwen/qwen3-8b:free\n"
     ]
    }
   ],
   "source": [
    "from embedded_utils import get_embedding_model\n",
    "embedding_model = get_embedding_model()\n",
    "\n",
    "model = \"qwen/qwen3-8b:free\"\n",
    "llm = ChatOpenAI(\n",
    "    model=model,\n",
    "    openai_api_key=api_key,\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    # default_headers={\n",
    "    #   \"HTTP-Referer\": \"http://localhost:8501\", # Replace with your actual site URL\n",
    "    #   \"X-Title\": \"My RAG App\", # Replace with your actual site name\n",
    "    # }\n",
    ")\n",
    "print(f\"LLM configured to use OpenRouter with model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "688866ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_fin_dict = {\n",
    "    'Risk Management': 'q-fin.RM',\n",
    "    'Computational Finance': 'q-fin.CP',\n",
    "    'Statistical Finance': 'q-fin.ST',\n",
    "    'Trading and Market Microstructure': 'q-fin.TR',\n",
    "    'Economics': 'q-fin.EC',\n",
    "    'General Finance': 'q-fin.GN',\n",
    "    'Mathematical Finance': 'q-fin.MF',\n",
    "    'Portfolio Management': 'q-fin.PM',\n",
    "    'Pricing of Securities': 'q-fin.PR'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50105d85",
   "metadata": {},
   "source": [
    "# Get Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6340edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_db_path = \"../vectorDB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1bdcfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../vectorDB/base/PrinciplesofFinance-WEB.pdf\n",
      "Loading vector store from: ../vectorDB/base/PrinciplesofFinance-WEB.pdf\n",
      "Vector store loaded successfully.\n",
      "Number of documents in store: 2137\n"
     ]
    }
   ],
   "source": [
    "persist_directory = 'base/PrinciplesofFinance-WEB.pdf'\n",
    "persist_directory = root_db_path + \"/\" + persist_directory\n",
    "\n",
    "print(persist_directory)\n",
    "print(f\"Loading vector store from: {persist_directory}\")\n",
    "\n",
    "try:\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embedding_model\n",
    "    )\n",
    "    print(\"Vector store loaded successfully.\")\n",
    "    # Optional: Check the number of documents in the store\n",
    "    print(f\"Number of documents in store: {vectorstore._collection.count()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading vector store: {e}\")\n",
    "    # Handle error, maybe the directory doesn't exist or is corrupted\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75bdefb",
   "metadata": {},
   "source": [
    "# Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8a48432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever created successfully.\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 3}\n",
    ")\n",
    "\n",
    "print(\"Retriever created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88743d",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "577b3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert assistant for answering questions.\n",
    "Use only the following context to answer the question at the end.\n",
    "If you don't know the answer from the context, just say that you don't know.\n",
    "Do not make up an answer.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387a37b",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6036cf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain created successfully. Ready to answer questions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# --- 5. Build the RAG Chain using LCEL ---\n",
    "\n",
    "# This function formats the retrieved documents into a single string\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# The core RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chain created successfully. Ready to answer questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794204dc",
   "metadata": {},
   "source": [
    "# Chain runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6ac476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 6. Run the Chain ---\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        question = input(\"Ask a question (or type 'exit' to quit): \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        # Invoke the chain to get the answer\n",
    "        # answer = rag_chain.invoke(question)\n",
    "        answer = retriever.invoke(question)\n",
    "        \n",
    "        print(\"\\n--- Answer ---\\n\")\n",
    "        for i, doc in enumerate(answer):\n",
    "            print(f\"\\n--- Document {i+1} ---\")\n",
    "            print(doc.page_content) \n",
    "        print(answer)\n",
    "        print(\"\\n--------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
